{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FINAL - OyezDataPrep.ipynb","provenance":[{"file_id":"16y-vpFnMxY9Ho6d7gPbnKpMLbyIdHIq7","timestamp":1622775121681},{"file_id":"1qCa-T3h0Y86OVYL14eusnvwUrhtR2VG3","timestamp":1622740622750},{"file_id":"1DiY6lY8h4715RBUfpTzdxRtq8tNT7u0a","timestamp":1622144544570},{"file_id":"1ZQTgWHvI4e4IKFJcMjeWe_em8P2cIV0j","timestamp":1621983202629},{"file_id":"1vrwtkrbyGLvfM_w4I1g4CLt0FZb5bmQc","timestamp":1621122140202},{"file_id":"1_rlpfAq28C64P_zEVixdrK0_TUVs4Bl0","timestamp":1621030817533}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"16y-vpFnMxY9Ho6d7gPbnKpMLbyIdHIq7","authorship_tag":"ABX9TyPg4RJdQWJdOmVZvTOzc6zg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0Ma42Am8i_AH"},"source":["## OyezDataPrep\n","\n","This notebook is exclusively for scraping and processing our real audio and TTS speech data. All the Deep Learning happens in \"FINAL - OyezTraining\""]},{"cell_type":"markdown","metadata":{"id":"lT8ist2F5g2R"},"source":["### References: \n","\n","Nemo: \n","\n","\n","*   https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/asr/01_ASR_with_NeMo.ipynb#scrollTo=7mP4r1Gx_Ilt\n","*   https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/tools/CTC_Segmentation_Tutorial.ipynb#scrollTo=hRFAl0gO92bp\n","* https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/tts/1_TTS_inference.ipynb#scrollTo=-BB2-KokaP08\n","*   https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/intro.html\n","\n","Please note our efforts usign these notebooks have already improved NeMo:\n","*   https://github.com/NVIDIA/NeMo/issues/2217#issuecomment-841738358\n","*   https://github.com/NVIDIA/NeMo/issues/2208\n","\n","Critically, we adapted an API from the Supreme Court diarization paper in our report:\n","* https://github.com/JeffT13/rd-diarization\n","\n","Thanks to Project Oyez for all the data: \n","* https://www.oyez.org/\n","\n","\n","\n","Note that outputs are cleared to minimize clutter"]},{"cell_type":"markdown","metadata":{"id":"eVTVzvW5k4Kr"},"source":["We load all dependencies and scripts we'll need from NeMo"]},{"cell_type":"code","metadata":{"id":"zH3pJK6JJmx6"},"source":["BRANCH = 'r1.0.0rc1'\n","\"\"\"\n","You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n","\n","Instructions for setting up Colab are as follows:\n","1. Open a new Python 3 notebook.\n","2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n","3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n","4. Run this cell to set up dependencies.\n","\"\"\"\n","# If you're using Google Colab and not running locally, run this cell.\n","# install NeMo\n","!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n","import json\n","import os\n","import wget\n","\n","from IPython.display import Audio\n","import numpy as np\n","import scipy.io.wavfile as wav\n","\n","! pip install pandas\n","\n","# optional\n","! pip install plotly\n","from plotly import graph_objects as go"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e4NC95sulaL"},"source":["# If you're running the notebook locally, update the TOOLS_DIR path below\n","# In Colab, a few required scripts will be downloaded from NeMo github\n","\n","import wget\n","TOOLS_DIR = '<UPDATE_PATH_TO_NeMo_root>/tools/ctc_segmentation/scripts'\n","\n","if 'google.colab' in str(get_ipython()):\n","    TOOLS_DIR = 'scripts/'\n","    os.makedirs(TOOLS_DIR, exist_ok=True)\n","\n","    required_files = ['prepare_data.py',\n","                    'normalization_helpers.py',\n","                    'run_ctc_segmentation.py',\n","                    'verify_segments.py',\n","                    'cut_audio.py',\n","                    'process_manifests.py',\n","                    'utils.py']\n","    for file in required_files:\n","        if not os.path.exists(os.path.join(TOOLS_DIR, file)):\n","            file_path = 'https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/tools/ctc_segmentation/' + TOOLS_DIR + file\n","            print(file_path)\n","            wget.download(file_path, TOOLS_DIR)\n","elif not os.path.exists(TOOLS_DIR):\n","      raise ValueError(f'update path to NeMo root directory')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_eMD0h84-tJ"},"source":["### 1. Scrape and segment Oyez audio"]},{"cell_type":"code","metadata":{"id":"EKG5EvY35Mjk"},"source":["## create data directory for the year we are scraping\n","WORK_DIR = 'WORK_DIR_2018'\n","DATA_DIR = WORK_DIR + '/DATA'\n","os.makedirs(DATA_DIR, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VowdpNKtsRS_"},"source":["This version, adapted from the Speaker Diarization link above, finds all cases for a year from Oyez API and formats them so they can be run through NeMO's CTC segmentation script. Some cases do not have transcripts, so the json fails and we move to next case"]},{"cell_type":"code","metadata":{"id":"YT6cx0NHxdzo"},"source":["from datetime import date\n","import traceback\n","\n","import json\n","\n","from urllib.request import urlopen\n","\n","import requests\n","#from ratelimit import limits, sleep_and_retry\n","\n","YEARS_TO_GO_BACK = 1\n","\n","\n","#\n","\n","def get_http_json(url):\n","    print(f\"Getting {url}\")\n","    response = requests.get(url)\n","    parsed = response.json()\n","    return parsed\n","\n","# this is the main function used, and it will download the audio file and create all the transcripts\n","# we get all the cases for a year and concatenate the json for each case and write it to a file\n","# then we pair it with it's audio file\n","def get_case(term, docket):\n","    \"\"\"Get the info of the case and fetch all\n","    transcripts that the info links to\"\"\"\n","    url = f\"https://api.oyez.org/cases/{term}/{docket}\"\n","    docket_data = get_http_json(url)\n","    print(docket_data[\"opinion_announcement\"])\n","    if (\"opinion_announcement\" in docket_data):\n","      opinion_announcement = docket_data[\"opinion_announcement\"]\n","      docket_number = docket_data[\"docket_number\"]\n","      t = opinion_announcement[0][\"href\"]\n","      audio_json = get_http_json(t)\n","      data =  urlopen(t).read()\n","      audio_link = audio_json[\"media_file\"][0][\"href\"]\n","      audio_file = audio_link[audio_link.rfind(\"/\")+1:]\n","      wget.download(audio_link, DATA_DIR)\n","      djsondict = json.loads(data)\n","      stop_time = 0\n","      full_text = \"\"\n","      for q in range(0,len(djsondict['transcript']['sections'])):\n","        for i in range(0,len(djsondict['transcript']['sections'][q]['turns'])):\n","          for j in range(0,len(djsondict['transcript']['sections'][q]['turns'][i]['text_blocks'])):\n","            temp_text = djsondict['transcript']['sections'][q]['turns'][i]['text_blocks'][j]['text'] + \"\\n\"\n","            temp_text = temp_text.replace('v.','v')\n","            full_text = full_text + temp_text\n","      with open(os.path.join(DATA_DIR, audio_file.replace('mp3', 'txt')), 'w') as f:\n","        f.write(full_text)\n","\n","\n","def write_case(term, docket, docket_data, transcripts):\n","    \"\"\"\n","    Writes term-docket.json file with docket_data\n","    For each transcript, writes the term-docket-t##.json file\n","    \"\"\"\n","    with open(f\"oyez/cases/{term}.{docket}.json\", \"w\") as docket_file:\n","        json.dump(docket_data, docket_file, indent=2)\n","\n","    count = 0\n","    for t in transcripts:\n","        count += 1\n","        t_filename = \"oyez/cases/{}.{}-t{:0>2d}.json\".format(term, docket, count)\n","        with open(t_filename, \"w\") as t_file:\n","            json.dump(t, t_file, indent=2)\n","\n","\n","# this calls the write_case function\n","def fetch_missing(cases):\n","    \"\"\"\n","    cases is a map of tuples to Summary (term, docket) : {SUMMARY}\n","    For each case, fetch the docket and transcript data and write to a file\n","    \n","    return set of cases that this was succesful for\n","    \"\"\"\n","    \n","    \n","    count = 0\n","    total = len(cases)\n","    succesful = set()\n","    for term, docket in cases.keys():\n","        ## pull the file\n","        count += 1\n","        print(term,docket)\n","        print(f\"Trying: {term}/{docket}\\t\\t{count}/{total}\")\n","        try:\n","            docket_data, transcripts = get_case(term, docket)\n","            if not transcripts:\n","                # No transcripts for this case yet\n","                continue\n","        except Exception as exc:\n","            traceback.print_exc()\n","            print(f\"Failed for {term}/{docket}, continuing anyways\")\n","    return succesful\n","\n","# this builds a dict with all the cases for the year specified\n","def find_missing(years):\n","    \"\"\"\n","    Fetch all summaries for given years and find any that are\n","    missing in the local \"known_map\"\n","    \"\"\"\n","    to_fetch = {}\n","    for year in years:\n","        summary_url = f\"https://api.oyez.org/cases?per_page=0&filter=term:{year}\"\n","        summaries = get_http_json(summary_url)\n","        for summary in summaries:\n","          to_fetch[(summary[\"term\"], summary[\"docket_number\"])] = summary\n","\n","    return to_fetch\n","\n","# this specifies the year to start at, for example our test set would start at 2018 and include 2017 so we'd look back 2 years\n","def years_to_recheck():\n","    \"\"\"\n","    Makes a list of years going back to\n","    YEARS_TO_GO_BACK\n","    e.g. [2018, 2019]\n","    \"\"\"\n","    cur_year = 2018\n","    return list(range(cur_year - YEARS_TO_GO_BACK + 1, cur_year + 1))\n","\n","# calls the functions above to download all audio and transcripts for 2018\n","def main():\n","    \"\"\"\n","    Find any cases that the server is updated with but we don't have locally\n","    and fetch the case info and transcripts for them.\n","    For all cases this is succesful for, also update case_summaries\n","    \"\"\"\n","    missing_summaries = find_missing(years_to_recheck())\n","\n","    print(f\"Missing {len(missing_summaries)} cases\")\n","    print(missing_summaries.keys())\n","\n","    fetch_missing(missing_summaries)\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-YMeac-wi5FV"},"source":["We create the folders needed for CTC segmentation and download NeMo's segmentation script"]},{"cell_type":"code","metadata":{"id":"T9BXBsBmvJax"},"source":["! mkdir $DATA_DIR/facts_data\n","! mkdir $DATA_DIR/text\n","! mkdir $DATA_DIR/audio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCMS0Fab0xBU"},"source":["if 'google.colab' in str(get_ipython()) and not os.path.exists('run_sample.sh'):\n","    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/tools/ctc_segmentation/run_sample.sh', '.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDsZfGtEs7Ji"},"source":["This script segments all case audio based on the transcript and creates the manfiests used for training and testing. One year of opinions take ~3 hours to run. "]},{"cell_type":"code","metadata":{"id":"Rog7w8VU0dQv"},"source":["MODEL = 'QuartzNet15x5Base-En'\n","OFFSET = 0\n","THRESHOLD = -5\n","if 'google.colab' in str(get_ipython()):\n","    OUTPUT_DIR_2 = f'/content/{WORK_DIR}/output_multiple_files'\n","else:\n","    OUTPUT_DIR_2 = os.path.join(WORK_DIR, 'output_multiple_files')\n","\n","! bash $TOOLS_DIR/../run_sample.sh \\\n","--MODEL_NAME_OR_PATH=$MODEL \\\n","--DATA_DIR=$DATA_DIR \\\n","--OUTPUT_DIR=$OUTPUT_DIR_2 \\\n","--SCRIPTS_DIR=$TOOLS_DIR \\\n","--CUT_PREFIX=0 \\\n","--MIN_SCORE=$THRESHOLD  \\\n","--USE_NEMO_NORMALIZATION=False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLuqEpHk6jc0"},"source":["Once we have run the above for each year in 2018 back to 2000, we can create a single manifest for or dev set (2015 and 2016) or our val set (2017, 2018). We found it easiest to manually move files over to Drive.\n","\n","Note that we exclude examples longer than 16 seconds, per the QuartzNet paper, to avoid CUDA memory errors"]},{"cell_type":"code","metadata":{"id":"UPDr5P56rE7S"},"source":["test_manifest_full_final = '/content/drive/MyDrive/Colab Notebooks/paired_final/test_manifest_full_final.json'\n","#create a new manifest with ONLY files shorter than 16 seconds from the test manifest\n","with open(test_manifest_full_final, 'w') as fout:\n","  for year in range(2015,2016):\n","    with open('/content/drive/MyDrive/Colab Notebooks/WORK_DIR_'+str(year)+'/output_multiple_files/all_manifest.json', 'r') as fin:\n","      for line in fin:\n","        # fix filepath\n","        line = line.replace(\"content/\",\"content/drive/MyDrive/Colab Notebooks/\")\n","        json_line = json.loads(line)\n","        file_to_check = json_line['audio_filepath']\n","        # make sure the filepath exists\n","        if path.exists(file_to_check) == True:\n","          duration = float(line[line.find(\"duration\")+10:line.find(',',line.find('duration'))])\n","          if duration <=16:\n","            fout.write(line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rvwaZRG64wJ"},"source":["Similarly, we can create a training set with all of our real paired data by executing the below.\n","\n","Please note later models were trained with just one year of paired data, so we would run the below with just in [2014] or in [2013,2014] to make those"]},{"cell_type":"code","metadata":{"id":"XnoYYqbhsF-R"},"source":["train_manifest_full_final = '/content/drive/MyDrive/Colab Notebooks/paired_final/train_manifest_full_final.json'\n","#create a new manifest with ONLY files shorter than 16 seconds from the test manifest\n","with open(train_manifest_full_final, 'w') as fout:\n","  for year in [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014]:\n","    with open('/content/drive/MyDrive/Colab Notebooks/WORK_DIR_'+str(year)+'/output_multiple_files/all_manifest.json', 'r') as fin:\n","      for line in fin:\n","        # fix filepath\n","        line = line.replace(\"content/\",\"content/drive/MyDrive/Colab Notebooks/\")\n","        json_line = json.loads(line)\n","        # make sure everything exists\n","        file_to_check = json_line['audio_filepath']\n","        if path.exists(file_to_check) == True:\n","          duration = float(line[line.find(\"duration\")+10:line.find(',',line.find('duration'))])\n","          if duration <=16:\n","            fout.write(line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vu9SstLz5TmT"},"source":["### 2. Scrape and format Oyez Hot Text\n"]},{"cell_type":"markdown","metadata":{"id":"VS51IIxo7S6O"},"source":["Now that we have our real audio, we need to download Oyez's accompanying \"facts of the case\" which will be our \"hot text\". The below will grab all HT for 2017 and 2018. Note for our dev set models, we would train on 2015,2016 hot text and for our val set models, we would grab 2017, 2018 hot text so that the text is highly overlapping with the real test set vocabulary"]},{"cell_type":"code","metadata":{"id":"_YW3WuqIvGvs"},"source":["## create data directory and download an audio file\n","WORK_DIR = 'WORK_DIR_2018_HT_nemo'\n","DATA_DIR = WORK_DIR + '/DATA'\n","os.makedirs(DATA_DIR, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49dj0J9_kSGk"},"source":["We modify the same functions above, adapted from the Speaker Diarization paper. RegEx stackoverflow cites:\n","* https://stackoverflow.com/questions/3398852/using-python-remove-html-tags-formatting-from-a-string\n","* https://stackoverflow.com/questions/40196941/regex-to-remove-periods-in-acronyms/40197005\n","* https://stackoverflow.com/questions/47625950/split-string-by-spaces-into-substrings-with-max-length-in-python/47626150"]},{"cell_type":"code","metadata":{"id":"V8wAEJIfvXTM"},"source":["# FOR TEXT SCRAPING\n","\n","from datetime import date\n","import traceback\n","import os\n","import json\n","import re\n","from urllib.request import urlopen\n","import textwrap\n","\n","import requests\n","#from ratelimit import limits, sleep_and_retry\n","\n","YEARS_TO_GO_BACK = 2\n","\n","def get_http_json(url):\n","    print(f\"Getting {url}\")\n","    response = requests.get(url)\n","    parsed = response.json()\n","    return parsed\n","\n","def striphtml(data):\n","    p = re.compile(r'<.*?>')\n","    return p.sub('', data)\n","\n","# Now this function is re-written to download the \"facts of the case\" from Oyez and writes it as a .txt file\n","# Note that we process the data using RegEx to delete punctuation, HTML, and periods after common abbreviations\n","def get_case(term, docket):\n","    \"\"\"Get the info of the case and fetch all\n","    transcripts that the info links to\"\"\"\n","    url = f\"https://api.oyez.org/cases/{term}/{docket}\"\n","    docket_data = get_http_json(url)\n","    print(\"here\")\n","    print(\"there\")\n","    facts = docket_data[\"facts_of_the_case\"]\n","    facts = re.sub(r'(?<!\\w)([A-Z])\\.', r'\\1', facts)\n","    facts = striphtml(facts)\n","    facts = re.sub('[^A-Za-z0-9- ?.]+', '', facts)\n","    facts = facts.replace('v.','v')\n","    facts = facts.replace('Mr.','Mr')\n","    facts = facts.replace('Mrs.','Mrs')\n","    facts = facts.replace('Ms.','Ms')\n","    facts = facts.replace(\"?\",\".\")\n","    question = docket_data[\"question\"]\n","    question = re.sub(r'(?<!\\w)([A-Z])\\.', r'\\1', question)\n","    question = striphtml(question)\n","    question = re.sub('[^A-Za-z0-9- ?.]+', '', question)\n","    question = question.replace('v.','v')\n","    question = question.replace('Mr.','Mr')\n","    question = question.replace('Mrs.','Mrs')\n","    question = question.replace('Ms.','Ms')\n","    question = question.replace(\"?\",\".\")\n","    conclusion = docket_data[\"conclusion\"]\n","    conclusion = re.sub(r'(?<!\\w)([A-Z])\\.', r'\\1', conclusion)\n","    conclusion = striphtml(conclusion)\n","    conclusion = re.sub('[^A-Za-z0-9- ?.]+', '', conclusion)\n","    conclusion = conclusion.replace('v.','v')\n","    conclusion = conclusion.replace('v.','v')\n","    conclusion = conclusion.replace('Mr.','Mr')\n","    conclusion = conclusion.replace('Mrs.','Mrs')\n","    conclusion = conclusion.replace('Ms.','Ms')\n","    conclusion = conclusion.replace(\"?\",\".\")\n","    facts = facts.split(\".\")\n","    question = question.split(\".\")\n","    conclusion = conclusion.split(\".\")\n","    print(facts)\n","    docket_number = docket_data[\"docket_number\"]\n","    # We save all the text to a text file for this case. We also wrap text at 100 characters\n","    # To avoid errors with audio that gets cutoff\n","    with open(os.path.join(DATA_DIR+'/facts_data/', docket_number + '.txt'), 'w') as f:\n","      for fact in facts:\n","        fact = fact.lstrip()\n","        fact_list = textwrap.wrap(fact, 100, break_long_words=False)\n","        for single_line in fact_list:\n","          f.write(single_line+'\\n')\n","      for q in question:\n","        q = q.lstrip()\n","        q_list = textwrap.wrap(q, 100, break_long_words=False)\n","        for single_line in q_list:\n","          f.write(single_line+'\\n')\n","      for conc in conclusion:\n","        conc = conc.lstrip()\n","        conc_list = textwrap.wrap(conc, 100, break_long_words=False)\n","        for single_line in conc_list:\n","          f.write(single_line+'\\n')\n","\n","\n","def write_case(term, docket, docket_data, transcripts):\n","    \"\"\"\n","    Writes term-docket.json file with docket_data\n","    For each transcript, writes the term-docket-t##.json file\n","    \"\"\"\n","    with open(f\"oyez/cases/{term}.{docket}.json\", \"w\") as docket_file:\n","        json.dump(docket_data, docket_file, indent=2)\n","\n","    count = 0\n","    for t in transcripts:\n","        count += 1\n","        t_filename = \"oyez/cases/{}.{}-t{:0>2d}.json\".format(term, docket, count)\n","        with open(t_filename, \"w\") as t_file:\n","            json.dump(t, t_file, indent=2)\n","\n","# this grabs the cases for the year specified from the dict\n","def fetch_missing(cases):\n","    \"\"\"\n","    cases is a map of tuples to Summary (term, docket) : {SUMMARY}\n","    For each case, fetch the docket and transcript data and write to a file\n","    \n","    return set of cases that this was succesful for\n","    \"\"\"\n","    \n","    \n","    count = 0\n","    total = len(cases)\n","    succesful = set()\n","    for term, docket in cases.keys():\n","        ## pull the file\n","        count += 1\n","        print(term,docket)\n","        print(f\"Trying: {term}/{docket}\\t\\t{count}/{total}\")\n","        try:\n","            docket_data, transcripts = get_case(term, docket)\n","            if not transcripts:\n","                # No transcripts for this case yet\n","                continue\n","        except Exception as exc:\n","            traceback.print_exc()\n","            print(f\"Failed for {term}/{docket}, continuing anyways\")\n","    return succesful\n","\n","# this builds a dict of all cases in the year specified\n","def find_missing(years):\n","    \"\"\"\n","    Fetch all summaries for given years and find any that are\n","    missing in the local \"known_map\"\n","    \"\"\"\n","    to_fetch = {}\n","    for year in years:\n","        summary_url = f\"https://api.oyez.org/cases?per_page=0&filter=term:{year}\"\n","        summaries = get_http_json(summary_url)\n","        for summary in summaries:\n","          to_fetch[(summary[\"term\"], summary[\"docket_number\"])] = summary\n","\n","    return to_fetch\n","\n","\n","def years_to_recheck():\n","    \"\"\"\n","    Makes a list of years going back to\n","    YEARS_TO_GO_BACK\n","    e.g. [2018, 2019]\n","    \"\"\"\n","    cur_year = 2018\n","    return list(range(cur_year - YEARS_TO_GO_BACK + 1, cur_year + 1))\n","\n","# runs the above functions to download all \"Facts of the case\" and write as .txt files\n","def main():\n","    \"\"\"\n","    Find any cases that the server is updated with but we don't have locally\n","    and fetch the case info and transcripts for them.\n","    For all cases this is succesful for, also update case_summaries\n","    \"\"\"\n","    missing_summaries = find_missing(years_to_recheck())\n","\n","    print(f\"Missing {len(missing_summaries)} cases\")\n","    print(missing_summaries.keys())\n","\n","    fetch_missing(missing_summaries)\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"554P7q8O7vFa"},"source":["### 3. Synthesize TTS from hot text"]},{"cell_type":"markdown","metadata":{"id":"I-0FJDe_7zk_"},"source":["Now we load NVIDIA's tts_tacotron2 model as our spectogram generator, and NVIDIA's tts_waveglow vocoder model which converts spectrograms to audio"]},{"cell_type":"code","metadata":{"id":"6BnZ7hsnUpXz"},"source":["import soundfile as sf\n","from nemo.collections.tts.models.base import SpectrogramGenerator, Vocoder\n","\n","# Download and load the pretrained tacotron2 model\n","spec_gen = SpectrogramGenerator.from_pretrained(\"tts_en_tacotron2\")\n","# Download and load the pretrained waveglow model\n","vocoder = Vocoder.from_pretrained(\"tts_waveglow_88m\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8Zdrhuj8CTp"},"source":["For every line in our text data, we synthesize speech. Note that the tacotron model first generates a spectrogram from the text, which the vocoder turns into audio.\n","\n","Note we also uncovered a bug in soundfile, which we submitted on GitHub: https://github.com/bastibe/python-soundfile/issues/203\n","\n","The below is adapted from: https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/tts/1_TTS_inference.ipynb#scrollTo=-BB2-KokaP08\n","\n","Stackoverflow cites:\n","* https://stackoverflow.com/questions/7099290/how-to-ignore-hidden-files-using-os-listdir"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uFCPPfN5U2lU"},"source":["from IPython.display import Audio #Import Audio method from IPython's Display Class\n","file_list = [f for f in os.listdir('/content/WORK_DIR_2014_HT_nemo/DATA/facts_data') if not f.startswith('.')] \n","print(file_list)\n","for file in file_list:\n","  counter=1\n","  with open(os.path.join(DATA_DIR+'/facts_data/'+file), 'r') as f:\n","    save_name = file[0:-4]\n","    for line in f:\n","      if len(line)>3:\n","        with open(os.path.join(DATA_DIR+'/text/'+save_name+'-'+str(counter)+'.txt'), 'w') as fout:\n","          fout.write(line)\n","        # the sprectrogram generator generates a spectrogram from the line of text\n","        parsed = spec_gen.parse(line)\n","        spectrogram = spec_gen.generate_spectrogram(tokens=parsed)\n","        # the vocoder conters this to audio\n","        audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n","        audio = audio.to('cpu').numpy()\n","        audio2 = audio.transpose()\n","        sf.write('/content/WORK_DIR_2014_HT_nemo/DATA/audio/'+save_name+'-'+str(counter)+'.wav', audio2, 22050)\n","        counter=counter+1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z31Zu4_q8bKQ"},"source":["We move all our text and audio files to Google Drive for safekeeping"]},{"cell_type":"code","metadata":{"id":"A691GYh8JzKz"},"source":["!mv '/content/WORK_DIR_2014_HT_nemo' '/content/drive/MyDrive/Colab Notebooks'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG5O14Rr_Lc8"},"source":["Adapted from: https://colab.research.google.com/github/NVIDIA/NeMo/blob/main/tutorials/asr/01_ASR_with_NeMo.ipynb#scrollTo=7mP4r1Gx_Ilt\n","\n","We now build a manifest with the \"hot text\" to match the format that NeMo needs.\n","\n","As written, the below code creates the \"hot text\" training manifest for our val model (2017 and 2018 data).\n","\n","To build the manifest for training against our dev set, we simply change the years to 2016 (that folder contains the hot text for 2015 and 2016)\n"]},{"cell_type":"code","metadata":{"id":"GpoDJny0_HQS"},"source":["# --- Building Manifest Files --- #\n","import librosa\n","import json\n","import os\n","\n","# Function to build a manifest\n","def build_manifest(transcripts_path, manifest_path):\n","    with open(manifest_path, 'w') as fout:\n","      for year in [2018]:\n","        # we go through every year in the HT folders and grab all the transcripts\n","        transcripts_path = transcripts_path.replace(\"2018\",str(year))\n","        file_list = [f for f in os.listdir(transcripts_path) if not f.startswith('.')] \n","        for t_file in file_list: \n","        # we read in each line from the transcript, make it lower case\n","        # we grab the audiofile path\n","        # we grab the duration\n","        # put all those together into one line in the manifest\n","        # then we go to the next transcript\n","         with open(transcripts_path+t_file, 'r') as fin:\n","            for line in fin:\n","                print(line)\n","                transcript = line.lower()\n","                audio_file = t_file.replace(\".txt\",\".wav\")\n","                audio_path = transcripts_path.replace(\"text\",\"audio\")+audio_file\n","                duration = librosa.core.get_duration(filename=audio_path)\n","\n","                # Write the metadata to the manifest\n","                metadata = {\n","                    \"audio_filepath\": audio_path,\n","                    \"duration\": duration,\n","                    \"text\": transcript\n","                }\n","                json.dump(metadata, fout)\n","                fout.write('\\n')\n","                \n","# Building Manifests\n","print(\"******\")\n","path_transcripts = '/content/drive/MyDrive/Colab Notebooks/WORK_DIR_2018_HT_nemo/DATA/text/'\n","path_manifest = '/content/drive/MyDrive/Colab Notebooks/TTS_paths/train_final.json'\n","build_manifest(path_transcripts, path_manifest)\n","print(\"Training manifest created.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqy01qAgAA6q"},"source":["Finally, we process the manifest to make sure it doesn't have extraneous text in the text line, and this creates a nice backup copy just in case something happens!\n","\n"]},{"cell_type":"code","metadata":{"id":"2ENu6vjjAAYM"},"source":["import json\n","with open('/content/drive/MyDrive/Colab Notebooks/TTS_manifests/train_for_val.json','r') as fin:\n","  with open('/content/drive/MyDrive/Colab Notebooks/TTS_manifests/train_for_val_corrected.json','w') as fout:\n","    for line in fin:\n","      json_line = json.loads(line)\n","      # we eliminate any extraneous end-of-line characters from our transcripts\n","      json_line[\"text\"] = json_line[\"text\"].replace(\"\\n\",\"\")\n","      json.dump(json_line,fout)\n","      fout.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v8msU6iP8YnC"},"source":["Finally, please note to create the mixed training manifests, e.g., 2 years of TTS and 1 year of paired data, we would manually copy and paste two manifests together using the above code to get the years right"]},{"cell_type":"markdown","metadata":{"id":"xKNIVmhA8gv9"},"source":["**Legacy code:**  we experimented with Google's TTS model as well, but found it had a slight accent and did not perform well as training data. Code is left below for completeness, but no Google TTS data appears in our final report"]},{"cell_type":"code","metadata":{"id":"ly8g8AFuvukZ"},"source":["import time\n","from gtts import gTTS #Import Google Text to Speech\n","from IPython.display import Audio #Import Audio method from IPython's Display Class\n","file_list = [f for f in os.listdir('/content/WORK_DIR_2014_HT_nemo/DATA/facts_data') if not f.startswith('.')] \n","print(file_list)\n","for file in file_list:\n","  counter=1\n","  with open(os.path.join(DATA_DIR+'/facts_data/'+file), 'r') as f:\n","    save_name = file[0:-4]\n","    for line in f:\n","      print(save_name)\n","      print(f)\n","      print(len(line))\n","      if len(line)>3:\n","        with open(os.path.join(DATA_DIR+'/text/'+save_name+str(counter)+'.txt'), 'w') as fout:\n","          fout.write(line)\n","        time.sleep(1.5)\n","        tts = gTTS(line) #Provide the string to convert to speech\n","        tts.save('/content/WORK_DIR_2014_HT_nemo/DATA/audio/'+save_name+str(counter)+'.wav') #save the string converted to speech as a .wav file\n","        counter=counter+1"],"execution_count":null,"outputs":[]}]}